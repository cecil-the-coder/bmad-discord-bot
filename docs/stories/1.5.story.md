# Story 1.5: API Usage Monitoring

## Status: Complete

## Story

- As the bot operator
- I want the application to internally track its usage of the Gemini API
- so that it can operate reliably without being rate-limited

## Acceptance Criteria (ACs)

* 1.5.1: The backend service must maintain an internal counter for Gemini API calls.
* 1.5.2: The counter should track usage over a configurable time window (e.g., requests per minute).
* 1.5.3: The system exposes an internal state representing the current usage level (e.g., Normal, Warning, Throttled).

## Tasks / Subtasks

- [x] Task 1: Implement Provider-Agnostic Rate Limit Data Model (AC: 1.5.1, 1.5.2)
  - [x] Create [`internal/monitor/ratelimiter.go`](internal/monitor/ratelimiter.go:1) with extensible `AIProviderRateLimiter` interface
  - [x] Implement `ProviderRateLimitState` struct supporting multiple time windows per provider
  - [x] Add `RegisterCall(providerID string)` method for provider-specific API usage tracking
  - [x] Implement `CleanupOldCalls(providerID string)` method for provider-specific sliding windows
  - [x] Add provider-specific configuration structure supporting different rate limit patterns
  - [x] Ensure thread-safety using `sync.RWMutex` for concurrent provider access

- [x] Task 2: Implement Multi-Provider Rate Limit Monitoring Service (AC: 1.5.1, 1.5.2, 1.5.3)
  - [x] Create `RateLimitManager` interface with provider-agnostic methods
  - [x] Implement `GetProviderUsage(providerID string) (int, int)` returning current usage for multiple windows
  - [x] Implement `GetProviderStatus(providerID string) string` returning Normal/Warning/Throttled per provider
  - [x] Add configurable provider registry supporting different rate limit configurations
  - [x] Create Gemini provider configuration as first concrete implementation
  - [x] Integrate go-cache v2.1.x with provider-specific cache keys for efficient tracking

- [x] Task 3: Integrate Provider-Aware Rate Limiting with AIService Interface (AC: 1.5.1)
  - [x] Extend [`AIService`](internal/service/ai_interface.go:1) interface to include provider identification and rate limit checking
  - [x] Modify [`GeminiCLIService`](internal/service/gemini_cli.go:1) to register as "gemini" provider and check limits before requests
  - [x] Add `GetProviderID() string` method to AIService interface for provider identification
  - [x] Add rate limit validation in `Query()` and `QueryWithContext()` methods using provider-specific limits
  - [x] Implement proper error handling for rate limit exceeded scenarios with provider context
  - [x] Maintain backward compatibility with existing AI service methods

- [x] Task 4: Environment Configuration for Multi-Provider Rate Limiting (AC: 1.5.2)
  - [x] Add provider-specific environment variables (AI_PROVIDER_GEMINI_RATE_LIMIT_PER_MINUTE=60, AI_PROVIDER_GEMINI_RATE_LIMIT_PER_DAY=1000)
  - [x] Update [`cmd/bot/main.go`](cmd/bot/main.go:1) to read provider configurations and initialize rate limit manager
  - [x] Add structured logging for provider-specific rate limit configuration validation
  - [x] Create provider registry with Gemini as default provider with specified limits
  - [x] Design configuration structure to easily add future providers (OpenAI, Claude, etc.)
  - [x] Ensure graceful degradation if rate limit monitoring fails for any provider

- [x] Task 5: Testing Implementation
  - [x] Create unit tests for `RateLimitState` struct methods and thread safety
  - [x] Create unit tests for rate limiter service with various usage scenarios
  - [x] Create integration tests for AI service rate limit integration
  - [x] Add tests for configuration validation and error scenarios
  - [x] Test edge cases: boundary conditions, concurrent access, time window transitions

## Dev Notes

### Previous Story Insights

From Story 1.4 implementation:
- AIService interface extension pattern is well-established and should be followed
- Environment configuration patterns in [`main.go`](cmd/bot/main.go:1) are consistent and reliable
- Structured logging with slog is implemented throughout the application
- Thread-safe operations are critical for Discord bot concurrent message handling
- Comprehensive error handling patterns are established for service operations

### Architecture Context

**Tech Stack Requirements**: [Source: architecture/tech-stack.md]
- Language: Golang 1.24.x for backend service development
- Cache: go-cache v2.1.x for in-memory API rate counter (thread-safe in-memory cache)
- Testing: Go Test 1.24.x for unit & integration testing
- Logging: slog 1.24.x for structured logging

**Project Structure**: [Source: architecture/source-tree.md]
- Rate limiter service: [`internal/monitor/ratelimiter.go`](internal/monitor/ratelimiter.go:1) (create new)
- AI service interface: [`internal/service/ai_interface.go`](internal/service/ai_interface.go:1) (extend existing)
- Gemini CLI implementation: [`internal/service/gemini_cli.go`](internal/service/gemini_cli.go:1) (extend existing)
- Main entry point: [`cmd/bot/main.go`](cmd/bot/main.go:1) (extend existing)

**Coding Standards**: [Source: architecture/coding-standards.md]
- **CRITICAL**: All business logic for AI interaction **MUST** use the `AIService` interface
- No direct calls to `gemini-cli` outside of `GeminiCLIService` implementation
- All code must be formatted with `gofmt`
- Secrets must only be read from environment variables at startup

**Data Models**: [Source: architecture/data-models.md + Future AI Provider Extensibility]
- **Provider-Agnostic RateLimitState specification**:
  ```go
  type ProviderRateLimitState struct {
      ProviderID    string                    // e.g., "gemini", "openai", "claude"
      TimeWindows   map[string][]time.Time    // e.g., "minute" -> timestamps, "day" -> timestamps
      Limits        map[string]int            // e.g., "minute" -> 60, "day" -> 1000
      Thresholds    map[string]float64        // e.g., "warning" -> 0.75, "throttled" -> 1.0
      Mutex         sync.RWMutex              // Read-write mutex for concurrent access
  }
  ```
- **AIProviderRateLimiter interface** for future extensibility to OpenAI, Claude, etc.
- No persistent database required for MVP - in-memory tracking with provider separation
- Thread-safety required using `sync.RWMutex` for concurrent provider access and Discord message handling

### Rate Limiting Implementation Details

**API Call Tracking**:
- Track timestamps of Gemini CLI calls in sliding time window
- Clean up old timestamps beyond the configured window automatically
- Provide real-time usage statistics for status determination

**Usage Level States**:
- **Normal**: Current usage below warning threshold (e.g., < 75% of limit)
- **Warning**: Usage approaching limit (e.g., 75-99% of limit)
- **Throttled**: Usage at or above configured limit (≥ 100% of limit)

**Configuration Requirements**:
- Environment variables for rate limit window duration and maximum calls
- Configurable thresholds for Warning and Throttled states
- Reasonable defaults to ensure bot functionality without explicit configuration

### Testing

Dev Note: Story Requires the following tests:

- [ ] Go Test Unit Tests: (nextToFile: true), coverage requirement: 80%
- [ ] Go Test Integration Test (Test Location): location: next to service implementations for rate limit integration validation

Manual Test Steps:
- Set BOT_TOKEN, GEMINI_CLI_PATH, AI_PROVIDER_GEMINI_RATE_LIMIT_PER_MINUTE=60, AI_PROVIDER_GEMINI_RATE_LIMIT_PER_DAY=1000
- Run `go run cmd/bot/main.go`
- Verify logs show "gemini" provider registration with correct rate limits
- Send 45+ rapid @mentions within 1 minute to trigger Warning state for Gemini provider (75% of 60/min limit)
- Send 60+ rapid @mentions within 1 minute to trigger Throttled state for Gemini provider (100% of 60/min limit)
- Verify rate limit tracking logs show provider-specific usage counts ("provider=gemini" in logs)
- Test provider-aware rate limit threshold detection (Normal → Warning → Throttled states)
- Verify bot continues to function normally under provider-specific rate limit monitoring
- Test concurrent usage tracking with multiple simultaneous questions for Gemini provider
- Test configuration extensibility by adding mock provider configuration (validate structure)

## Dev Agent Record

### Agent Model Used: {{Agent Model Name/Version}}

### Debug Log References

[[LLM: (Dev Agent) If the debug is logged to during the current story progress, create a table with the debug log and the specific task section in the debug log - do not repeat all the details in the story]]

### Completion Notes List

[[LLM: (Dev Agent) Anything the SM needs to know that deviated from the story that might impact drafting the next story.]]

### File List

- [`internal/monitor/ratelimiter.go`](internal/monitor/ratelimiter.go:1)
- [`internal/monitor/ratelimiter_test.go`](internal/monitor/ratelimiter_test.go:1)
- [`internal/service/ai_interface.go`](internal/service/ai_interface.go:1)
- [`internal/service/gemini_cli.go`](internal/service/gemini_cli.go:1)
- [`internal/service/gemini_cli_test.go`](internal/service/gemini_cli_test.go:1)
- [`cmd/bot/main.go`](cmd/bot/main.go:1)
- [`docs/stories/1.5.story.md`](docs/stories/1.5.story.md:1)

### Change Log

[[LLM: (Dev Agent) Track document versions and changes during development that deviate from story dev start]]

| Date | Version | Description | Author |
| :--- | :------ | :---------- | :----- |

## QA Results

[[LLM: QA Agent Results]]